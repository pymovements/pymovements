{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "221eb218",
   "metadata": {},
   "source": [
    "# Experiment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc670de",
   "metadata": {},
   "source": [
    "The information about the experimental setup in which the eye-tracking data was collected plays a crucial role in its processing. Screen size, viewing distance, coordinate origin, and sampling rate all determine how pixel\n",
    "positions translate into physical or visual units.\n",
    "\n",
    "In `pymovements`, this contextual information is stored in an {py:class}`~pymovements.Experiment` object, which describes the **geometry and timing of the recording setup**. Providing an experiment is not strictly required to load gaze data: samples can still be inspected in raw pixel coordinates and timestamps without it (see {doc}`Working with Raw Gaze Samples <inspect-raw-samples>`). \n",
    "\n",
    "However, the experiment definition becomes essential as soon as gaze samples are interpreted in physical or visual terms, for instance, converting pixels to degrees of visual angle ({py:func}`~pymovements.gaze.transforms.pix2deg`) or computing velocities ({py:func}`~pymovements.gaze.transforms.pos2vel`). These conversions might be necessary for detecting oculomotor events such as fixations and saccades ({doc}`Events </reference/events>`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c554bbd6",
   "metadata": {},
   "source": [
    "## Creating an Experiment\n",
    "\n",
    "The most common way to create an experiment is by directly providing screen and sampling parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pymovements as pm\n",
    "from pymovements import Experiment\n",
    "\n",
    "example_experiment = Experiment(\n",
    "    screen_width_px=1280,\n",
    "    screen_height_px=1024,\n",
    "    screen_width_cm=38.0,\n",
    "    screen_height_cm=30.0,\n",
    "    distance_cm=68.0,\n",
    "    origin=\"upper left\",\n",
    "    sampling_rate=1000.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6faf1",
   "metadata": {},
   "source": [
    "### Variable Viewing Distance\n",
    "\n",
    "In many laboratory setups, the eye-to-screen distance is constant and can be specified once via `distance_cm` in the experiment definition. However, in some recordings, for example in head-free or mobile eye-tracking setups, the distance between the eye and the screen may vary over time. In such cases, the viewing distance can be provided per sample as a column in the {py:class}`~pymovements.Gaze` data instead of as a fixed value in the experiment. The eye-to-screen distance is required for converting pixel coordinates into degrees of visual angle (dva). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8bea9",
   "metadata": {},
   "source": [
    "### Coordinate Origin\n",
    "\n",
    "The `origin` parameter defines where pixel coordinate (0, 0) is located on the screen. This choice affects how gaze positions are interpreted and transformed. The origin should match the coordinate system used during stimulus presentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01bc10",
   "metadata": {},
   "source": [
    "### Sampling Rate\n",
    "\n",
    "The sampling rate determines the temporal resolution of the recording and is stored in the eye tracker component of the experiment. Typically reported in hertz (Hz), it specifies how many gaze samples are recorded per second. Values range from around 30 Hz for low-cost or webcam-based systems to 500 Hz or 2000 Hz for high-end research-grade eye trackers.\n",
    "\n",
    "In general, higher sampling rates allow finer temporal detail to be captured. The Nyquist–Shannon sampling theorem states that a signal must be sampled at least twice as fast as its highest frequency component to avoid aliasing. In practice, this means that high sampling rates are required to capture rapid eye movements such as saccades, while lower sampling rates may be sufficient for analyses focused on fixations or overall viewing patterns.\n",
    "\n",
    "The sampling rate value is required for any operation that involves time derivatives, such as computing velocity, i.e. change in position over time. It is accessible via `experiment.sampling_rate` as seen below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_experiment.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d93a0d",
   "metadata": {},
   "source": [
    "## Screen and EyeTracker Objects\n",
    "\n",
    "The instantiation of an ``Experiment`` creates two more objects internally, which constitute its main components. These are  {py:class}`~pymovements.Screen` and an {py:class}`~pymovements.EyeTracker`. We can find them by examining the created {py:class}`~pymovements.Experiment` object below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c919c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d42e64",
   "metadata": {},
   "source": [
    "### Screen\n",
    "\n",
    "The {py:class}`~pymovements.Screen` class describes the physical and pixel properties of the display used during recording. These parameters determine how gaze positions in pixels map to real-world or visual-angle units. They include:\n",
    "\n",
    "- screen resolution in pixels (`width_px`, `height_px`)\n",
    "- physical screen size in centimeters (`width_cm`, `height_cm`)\n",
    "- eye-to-screen distance (`distance_cm`)\n",
    "- coordinate origin (e.g., `\"upper left\"` or `\"center\"`)\n",
    "\n",
    "If all physical parameters (`width_cm`, `height_cm`, `distance_cm`) and pixel resolution are provided, screen coordinates can be converted from pixels to degrees of visual angle (dva, °). The screen boundaries in dva are available via the properties `x_min_dva`, `x_max_dva`, `y_min_dva`, and `y_max_dva`.\n",
    "\n",
    "### Eye Tracker\n",
    "\n",
    "The {py:class}`~pymovements.EyeTracker` class stores temporal and device-specific properties of the eye tracker model. It contains:\n",
    "\n",
    "- sampling rate in Hz\n",
    "- which eye(s) were recorded\n",
    "- optional metadata about the device\n",
    "\n",
    "The sampling rate is especially important for time-based computations such as velocity estimation and event detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a885c91",
   "metadata": {},
   "source": [
    "Both {py:class}`~pymovements.Screen` and {py:class}`~pymovements.EyeTracker` objects can be created separately and then combined into an {py:class}`~pymovements.Experiment`. This is useful when screen or device properties are reused across datasets or need to be modified independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a216c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = pm.Screen(\n",
    "    width_px=1920,\n",
    "    height_px=1080,\n",
    "    width_cm=53.0,\n",
    "    height_cm=30.0,\n",
    "    distance_cm=65.0,\n",
    "    origin=\"upper left\",\n",
    ")\n",
    "\n",
    "eyetracker = pm.EyeTracker(\n",
    "    sampling_rate=1000.0,\n",
    "    model=\"EyeLink 1000 Plus\",\n",
    "    left=False,\n",
    "    right=True,\n",
    "    version=\"2.0\",\n",
    "    vendor=\"EyeLink\",\n",
    "    mount=\"Arm Mount / Monocular / Remote\")\n",
    "\n",
    "experiment = pm.Experiment(screen=screen, eyetracker=eyetracker)\n",
    "\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638f906",
   "metadata": {},
   "source": [
    "## Loading Experiment from a Dictionary\n",
    "\n",
    "Experiments can also be constructed from dictionaries, which is useful when\n",
    "reading metadata from configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddd462",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment.from_dict({\n",
    "    \"screen\": {\n",
    "        \"width_px\": 1280,\n",
    "        \"height_px\": 1024,\n",
    "        \"width_cm\": 38.0,\n",
    "        \"height_cm\": 30.0,\n",
    "        \"distance_cm\": 68.0,\n",
    "        \"origin\": \"upper left\",\n",
    "    },\n",
    "    \"eyetracker\": {\n",
    "        \"sampling_rate\": 1000.0,\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df51276",
   "metadata": {},
   "source": [
    "## Saving Experiment Settings\n",
    "\n",
    "Experiment configurations can be exported to a dictionary or `YAML` file for\n",
    "reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04dffe",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "experiment_dict = experiment.to_dict()\n",
    "experiment.to_yaml(\"experiment.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2176a2",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# remove the YAML file\n",
    "os.remove(\"experiment.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233b80c",
   "metadata": {},
   "source": [
    "\n",
    "This makes it easy to store the recording setup alongside your data and reuse it in later analyses.\n",
    "\n",
    "In the next section, we will use this experiment definition while loading gaze data and explore how raw samples are represented inside `pymovements`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
