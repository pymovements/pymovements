{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b91b54",
   "metadata": {},
   "source": [
    "# Working with Raw Gaze Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb9eed",
   "metadata": {},
   "source": [
    "Once gaze data have been loaded, they are available as time-ordered raw samples in `gaze.samples`. \n",
    "The term \"raw gaze data\" is used inconsistently in the literature. It may refer to:\n",
    "\n",
    "- Original eye-tracker files containing mixed content (samples, events, messages)\n",
    "- Gaze coordinates over time without filtering or event classification\n",
    "- Vendor-provided event labels (e.g., fixations or saccades)\n",
    "\n",
    "In `pymovements`, \"raw samples\" refer specifically to the lowest-level gaze time series available after import, before smoothing, velocity computation, or event detection. All higher-level measures are derived from these samples.\n",
    "\n",
    "The table below shows a simplified example of raw gaze samples after import ({py:class}`~pymovements.Gaze`). Each row corresponds to one time-ordered gaze sample and is stored in the ``samples`` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e823cc",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-stderr"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pymovements as pm\n",
    "from pymovements.gaze.experiment import Experiment\n",
    "\n",
    "experiment = Experiment(\n",
    "    screen_width_px=1680,\n",
    "    screen_height_px=1050,\n",
    "    screen_width_cm=47.5,\n",
    "    screen_height_cm=30,\n",
    "    distance_cm=65,\n",
    "    origin='upper left',\n",
    "    sampling_rate=1000.0,\n",
    ")\n",
    "\n",
    "gaze_0 = pm.gaze.from_csv(\n",
    "    \"../examples/gaze-toy-example.csv\",\n",
    "    experiment=experiment,\n",
    "    time_column=\"timestamp\",\n",
    ")\n",
    "\n",
    "gaze_0.samples.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99372751",
   "metadata": {},
   "source": [
    "Column dtypes after import:\n",
    "\n",
    "- ``time`` (``i64``): timestamp of the sample, typically in milliseconds.\n",
    "- ``x`` (``f64``): horizontal gaze position in pixel coordinates.\n",
    "- ``y`` (``f64``): vertical gaze position in pixel coordinates.\n",
    "- `stimuli_x` and `stimuli_y` describe the position of the currently presented stimulus on the screen at each time point. These values are typically given in pixel coordinates and allow gaze samples to be related directly to the visual content shown to the participant. If no stimulus position was available at a given time, these fields may contain placeholder values (for example `-1`) indicating the absence of an active stimulus.\n",
    "\n",
    "Depending on the loader and input format, additional channels such as binocular coordinates or quality measures may also be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b55c74",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "gaze = pm.gaze.from_csv(\n",
    "    \"../examples/gaze-toy-example.csv\",\n",
    "    experiment=experiment,\n",
    "    time_column=\"timestamp\",\n",
    "    pixel_columns=['x', 'y']\n",
    ")\n",
    "\n",
    "# drop columns stimuli_x and stimuli_y\n",
    "gaze.samples = gaze.samples.drop(['stimuli_x', 'stimuli_y'])\n",
    "gaze.samples.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410aa2f",
   "metadata": {},
   "source": [
    "## Inspecting Raw Samples with Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eae4df",
   "metadata": {},
   "source": [
    "Visual inspection is an essential first step when working with newly loaded gaze data. Time-series plots help reveal signal loss, noise, blinks, sampling irregularities, or calibration problems before any preprocessing is applied. \n",
    "\n",
    "Using the {py:func}`~pymovements.plotting.traceplot` function, we can visualize raw gaze samples from a {py:class}`~pymovements.Gaze` object. The plot shows the continuous trajectory of gaze positions across the stimulus, allowing inspection of spatial gaze behavior over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fe9dc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "pm.plotting.traceplot(gaze)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42a408",
   "metadata": {},
   "source": [
    "The {py:func}`~pymovements.plotting.tsplot` function produces a time-series plot which shows how each recorded signal changes over time, with one line per selected channel (e.g., horizontal and vertical gaze position). The x-axis represents time, as defined by the gaze sample timestamps.\n",
    "\n",
    "In this example, we plot the `x` and `y` channels to examine the raw gaze signal before applying any event detection or preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334630bc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "pm.plotting.tsplot(\n",
    "    gaze_0,\n",
    "    channels=['x', 'y'],\n",
    "    share_y=False,\n",
    "    line_color=\"darkblue\",\n",
    "    zero_centered_yaxis=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9371f",
   "metadata": {},
   "source": [
    "See the {doc}`Plotting Gaze Data tutorial <../tutorials/plotting>` for an example of time-series visualization using ``traceplot``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507134e",
   "metadata": {},
   "source": [
    "## Transforming Raw Samples\n",
    "\n",
    "Raw pixel coordinates are tied to a specific screen setup and viewing distance. For meaningful interpretation and cross-experiment comparison, gaze samples are often transformed into alternative representations. These transformations operate directly on the raw samples and rely on the experimental metadata defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454bc49",
   "metadata": {},
   "source": [
    "### From Pixels to Degrees of Visual Angle\n",
    "\n",
    "Eye trackers typically record gaze positions in screen pixels. While useful for display-based inspection, pixel units depend on screen size and viewing distance and are therefore not comparable across setups. The function {py:func}`~pymovements.gaze.transforms.pix2deg` converts pixel coordinates into degrees of visual angle (dva) using the experiment's screen geometry and viewing distance.\n",
    "\n",
    "Requirements:\n",
    "- A pixel-based gaze column must be available (by default named \"pixel\")\n",
    "- An {py:class}`~pymovements.Experiment` must be attached to the gaze data, because screen size and distance are needed for the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze.pix2deg()\n",
    "\n",
    "gaze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e04e36",
   "metadata": {},
   "source": [
    "### From Position to Velocity\n",
    "\n",
    "Many eye-movement measures are derived not from position directly but from its temporal\n",
    "derivatives. Velocity is computed from changes in gaze position over time and is central to event detection algorithms for saccades and fixations. In pymovements, velocity is computed explicitly from position data with the {py:func}`~pymovements.gaze.transforms.pos2vel` function, using the sampling rate stored in the eye tracker definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze.pos2vel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
