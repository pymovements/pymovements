{
 "cells": [
  {
   "cell_type": "raw",
   "id": "17b91b54",
   "metadata": {},
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"raw\",\n",
    "   \"id\": \"17b91b54\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"{\\n\",\n",
    "    \" \\\"cells\\\": [\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"17b91b54\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"# Working with Raw Gaze Samples\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"7abb9eed\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"Once gaze data have been loaded, they are available as time-ordered raw samples in `gaze.samples`. \\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"The table below shows a basic example of raw gaze samples after import into `pymovemnts`. Each row corresponds to one time-ordered gaze sample and is stored in the ``samples`` attribute of the {py:class}`~pymovements.Gaze` object. Timestamps are listed in the ``time`` column, and horizontal and vertical gaze positions in pixel coordinates can be found in the `pixel` column. Depending on the loader and input format, additional channels such as binocular coordinates or quality measures may also be present.\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"execution_count\\\": null,\\n\",\n",
    "    \"   \\\"id\\\": \\\"e4e823cc\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"tags\\\": [\\n\",\n",
    "    \"     \\\"hide-input\\\",\\n\",\n",
    "    \"     \\\"remove-stderr\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"import matplotlib.pyplot as plt\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"import pymovements as pm\\\\n\\\",\\n\",\n",
    "    \"    \\\"from pymovements.gaze.experiment import Experiment\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"experiment = Experiment(\\\\n\\\",\\n\",\n",
    "    \"    \\\"    screen_width_px=1280,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    screen_height_px=1024,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    screen_width_cm=38,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    screen_height_cm=30.2,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    distance_cm=68,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    origin='upper left',\\\\n\\\",\\n\",\n",
    "    \"    \\\"    sampling_rate=250.0,\\\\n\\\",\\n\",\n",
    "    \"    \\\")\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"gaze = pm.gaze.from_csv(\\\\n\\\",\\n\",\n",
    "    \"    \\\"    '../examples/gaze-toy-example.csv',\\\\n\\\",\\n\",\n",
    "    \"    \\\"    experiment=experiment,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    time_column='time',\\\\n\\\",\\n\",\n",
    "    \"    \\\"    pixel_columns=['x', 'y'],\\\\n\\\",\\n\",\n",
    "    \"    \\\")\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"gaze.samples.head(5)\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"8410aa2f\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"## Inspecting Raw Samples with Plots\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"f2eae4df\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"Visual inspection is an essential first step when working with newly loaded gaze data. Time-series plots help reveal signal loss, noise, blinks, sampling irregularities, or calibration problems before any preprocessing is applied. Using the {py:func}`~pymovements.plotting.traceplot` function, we can visualize raw gaze samples from a {py:class}`~pymovements.Gaze` object. The plot shows the continuous trajectory of gaze positions across the stimulus, allowing inspection of spatial gaze behavior over time.\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"execution_count\\\": null,\\n\",\n",
    "    \"   \\\"id\\\": \\\"1c5fe9dc\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"tags\\\": [\\n\",\n",
    "    \"     \\\"hide-input\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"pm.plotting.traceplot(gaze)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"plt.show()\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"af42a408\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"We can examine how each recorded signal changes over time by using the {py:func}`~pymovements.plotting.tsplot` function. It produces a time-series plot with one line per selected channel (e.g., horizontal and vertical gaze position). The x-axis represents time, as defined by the gaze sample timestamps. In this example, we plot the `x` and `y` channels. \\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"execution_count\\\": null,\\n\",\n",
    "    \"   \\\"id\\\": \\\"334630bc\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"tags\\\": [\\n\",\n",
    "    \"     \\\"hide-input\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"gaze_unnested = gaze.clone()\\\\n\\\",\\n\",\n",
    "    \"    \\\"gaze_unnested.unnest('pixel')\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"pm.plotting.tsplot(\\\\n\\\",\\n\",\n",
    "    \"    \\\"    gaze_unnested,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    channels=['pixel_x', 'pixel_y'],\\\\n\\\",\\n\",\n",
    "    \"    \\\"    share_y=False,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    line_color='darkblue',\\\\n\\\",\\n\",\n",
    "    \"    \\\"    zero_centered_yaxis=False,\\\\n\\\",\\n\",\n",
    "    \"    \\\")\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"plt.show()\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"b507134e\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"## Transforming Raw Samples\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"Raw pixel coordinates are tied to a specific screen setup and viewing distance. For meaningful interpretation and cross-experiment comparison, gaze samples are often transformed into alternative representations. These transformations operate directly on the raw samples and rely on the experimental metadata defined earlier.\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"b454bc49\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"### `pix2deg()`: From Pixels to Degrees of Visual Angle\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"Eye trackers typically record gaze positions in screen pixels. While useful for display-based inspection, pixel units depend on screen size and viewing distance and are therefore not comparable across setups. The {py:func}`~pymovements.gaze.transforms.pix2deg` function converts pixel coordinates into degrees of visual angle (dva) using the experiment's screen geometry and viewing distance.\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"Requirements:\\\\n\\\",\\n\",\n",
    "    \"    \\\"- A pixel-based gaze column must be available (by default named \\\\\\\"pixel\\\\\\\")\\\\n\\\",\\n\",\n",
    "    \"    \\\"- An {py:class}`~pymovements.Experiment` must be attached to the gaze data, because screen size and distance are needed for the conversion\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"execution_count\\\": null,\\n\",\n",
    "    \"   \\\"id\\\": \\\"53adbc90\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"gaze.pix2deg()\\\\n\\\",\\n\",\n",
    "    \"    \\\"gaze\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"6550514e\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"Unlike pixels, degrees of visual angle reflect the actual angular displacement of the eye relative to the observer's viewpoint and are therefore comparable across different screen setups and viewing distances. In the plot below, the overall shape of the signal remains the same as in pixel space, since only the unit of measurement has changed. However, the scale of the y-axes differs, reflecting the conversion from screen-dependent coordinates to angular units.\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"execution_count\\\": null,\\n\",\n",
    "    \"   \\\"id\\\": \\\"b9b9137f\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"tags\\\": [\\n\",\n",
    "    \"     \\\"hide-input\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"gaze_unnested = gaze.clone()\\\\n\\\",\\n\",\n",
    "    \"    \\\"gaze_unnested.unnest('position')\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"pm.plotting.tsplot(\\\\n\\\",\\n\",\n",
    "    \"    \\\"    gaze_unnested,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    channels=['position_x', 'position_y'],\\\\n\\\",\\n\",\n",
    "    \"    \\\"    share_y=False,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    line_color='darkblue',\\\\n\\\",\\n\",\n",
    "    \"    \\\"    n_rows=2,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    n_cols=1,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    zero_centered_yaxis=False,\\\\n\\\",\\n\",\n",
    "    \"    \\\")\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"plt.show()\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"73e04e36\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"### `pos2vel()`: From Position to Velocity\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"Many eye-movement measures are derived not from position directly but from its temporal\\\\n\\\",\\n\",\n",
    "    \"    \\\"derivatives. Velocity is computed from changes in gaze position over time and is central to event detection algorithms for saccades and fixations. In pymovements, velocity is computed explicitly from position data with the {py:func}`~pymovements.gaze.transforms.pos2vel` function, using the sampling rate stored in the eye tracker definition.\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"execution_count\\\": null,\\n\",\n",
    "    \"   \\\"id\\\": \\\"51a8e663\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"gaze.pos2vel()\\\\n\\\",\\n\",\n",
    "    \"    \\\"gaze\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"ad4b3763\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"The following plot illustrates velocity, i.e. how quickly the eye moves at each time point. Periods of relative stability (low velocity) typically correspond to fixations, whereas sharp peaks in the signal indicate rapid eye movements such as saccades.\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"execution_count\\\": null,\\n\",\n",
    "    \"   \\\"id\\\": \\\"1845b738\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"tags\\\": [\\n\",\n",
    "    \"     \\\"hide-input\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"gaze_unnested = gaze.clone()\\\\n\\\",\\n\",\n",
    "    \"    \\\"gaze_unnested.unnest('velocity')\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"pm.plotting.tsplot(\\\\n\\\",\\n\",\n",
    "    \"    \\\"    gaze_unnested,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    channels=['velocity_x', 'velocity_y'],\\\\n\\\",\\n\",\n",
    "    \"    \\\"    share_y=False,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    line_color='darkblue',\\\\n\\\",\\n\",\n",
    "    \"    \\\"    n_rows=2,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    n_cols=1,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    zero_centered_yaxis=False,\\\\n\\\",\\n\",\n",
    "    \"    \\\")\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"plt.show()\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"markdown\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"203904c6\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"For more information on these preprocessing steps, please see the {doc}`Preprocessing Raw Gaze Data <../tutorials/preprocessing-raw-data>`\\\"\\n\",\n",
    "    \"   ]\\n\",\n",
    "    \"  }\\n\",\n",
    "    \" ],\\n\",\n",
    "    \" \\\"metadata\\\": {},\\n\",\n",
    "    \" \\\"nbformat\\\": 4,\\n\",\n",
    "    \" \\\"nbformat_minor\\\": 5\\n\",\n",
    "    \"}\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"e4e823cc\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\",\n",
    "     \"remove-stderr\"\n",
    "    ]\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pymovements as pm\\n\",\n",
    "    \"from pymovements.gaze.experiment import Experiment\\n\",\n",
    "    \"\\n\",\n",
    "    \"experiment = Experiment(\\n\",\n",
    "    \"    screen_width_px=1280,\\n\",\n",
    "    \"    screen_height_px=1024,\\n\",\n",
    "    \"    screen_width_cm=38,\\n\",\n",
    "    \"    screen_height_cm=30.2,\\n\",\n",
    "    \"    distance_cm=68,\\n\",\n",
    "    \"    origin='upper left',\\n\",\n",
    "    \"    sampling_rate=250.0,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"gaze = pm.gaze.from_csv(\\n\",\n",
    "    \"    '../examples/gaze-toy-example.csv',\\n\",\n",
    "    \"    experiment=experiment,\\n\",\n",
    "    \"    time_column='time',\\n\",\n",
    "    \"    pixel_columns=['x', 'y']\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"gaze.samples.head(5)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"f2eae4df\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Visual inspection is an essential first step when working with newly loaded gaze data. Time-series plots help reveal signal loss, noise, blinks, sampling irregularities, or calibration problems before any preprocessing is applied. Using the {py:func}`~pymovements.plotting.traceplot` function, we can visualize raw gaze samples from a {py:class}`~pymovements.Gaze` object. The plot shows the continuous trajectory of gaze positions across the stimulus, allowing inspection of spatial gaze behavior over time.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"af42a408\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"We can examine how each recorded signal changes over time by using the {py:func}`~pymovements.plotting.tsplot` function. It produces a time-series plot with one line per selected channel (e.g., horizontal and vertical gaze position). The x-axis represents time, as defined by the gaze sample timestamps. In this example, we plot the `x` and `y` channels. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"b507134e\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Transforming Raw Samples\\n\",\n",
    "    \"\\n\",\n",
    "    \"Raw pixel coordinates are tied to a specific screen setup and viewing distance. For meaningful interpretation and cross-experiment comparison, gaze samples are often transformed into alternative representations. These transformations operate directly on the raw samples and rely on the experimental metadata defined earlier.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"53adbc90\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"gaze.pix2deg()\\n\",\n",
    "    \"gaze\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"b9b9137f\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\"\n",
    "    ]\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"gaze_unnested = gaze.clone()\\n\",\n",
    "    \"gaze_unnested.unnest('position')\\n\",\n",
    "    \"\\n\",\n",
    "    \"pm.plotting.tsplot(\\n\",\n",
    "    \"    gaze_unnested,\\n\",\n",
    "    \"    channels=['position_x', 'position_y'],\\n\",\n",
    "    \"    share_y=False,\\n\",\n",
    "    \"    line_color=\\\"darkblue\\\",\\n\",\n",
    "    \"    n_rows=2, n_cols=1,\\n\",\n",
    "    \"    zero_centered_yaxis=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"51a8e663\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"gaze.pos2vel()\\n\",\n",
    "    \"gaze\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"1845b738\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\"\n",
    "    ]\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"gaze_unnested = gaze.clone()\\n\",\n",
    "    \"gaze_unnested.unnest('velocity')\\n\",\n",
    "    \"\\n\",\n",
    "    \"pm.plotting.tsplot(\\n\",\n",
    "    \"    gaze_unnested,\\n\",\n",
    "    \"    channels=['velocity_x', 'velocity_y'],\\n\",\n",
    "    \"    share_y=False,\\n\",\n",
    "    \"    line_color=\\\"darkblue\\\",\\n\",\n",
    "    \"    n_rows=2, n_cols=1,\\n\",\n",
    "    \"    zero_centered_yaxis=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {},\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e823cc",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-stderr"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pymovements as pm\n",
    "from pymovements.gaze.experiment import Experiment\n",
    "\n",
    "experiment = Experiment(\n",
    "    screen_width_px=1280,\n",
    "    screen_height_px=1024,\n",
    "    screen_width_cm=38,\n",
    "    screen_height_cm=30.2,\n",
    "    distance_cm=68,\n",
    "    origin='upper left',\n",
    "    sampling_rate=250.0,\n",
    ")\n",
    "\n",
    "gaze = pm.gaze.from_csv(\n",
    "    '../examples/gaze-toy-example.csv',\n",
    "    experiment=experiment,\n",
    "    time_column='time',\n",
    "    pixel_columns=['x', 'y'],\n",
    ")\n",
    "\n",
    "gaze.samples.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eae4df",
   "metadata": {},
   "source": [
    "Visual inspection is an essential first step when working with newly loaded gaze data. Time-series plots help reveal signal loss, noise, blinks, sampling irregularities, or calibration problems before any preprocessing is applied. Using the {py:func}`~pymovements.plotting.traceplot` function, we can visualize raw gaze samples from a {py:class}`~pymovements.Gaze` object. The plot shows the continuous trajectory of gaze positions across the stimulus, allowing inspection of spatial gaze behavior over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42a408",
   "metadata": {},
   "source": [
    "We can examine how each recorded signal changes over time by using the {py:func}`~pymovements.plotting.tsplot` function. It produces a time-series plot with one line per selected channel (e.g., horizontal and vertical gaze position). The x-axis represents time, as defined by the gaze sample timestamps. In this example, we plot the `x` and `y` channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507134e",
   "metadata": {},
   "source": [
    "## Transforming Raw Samples\n",
    "\n",
    "Raw pixel coordinates are tied to a specific screen setup and viewing distance. For meaningful interpretation and cross-experiment comparison, gaze samples are often transformed into alternative representations. These transformations operate directly on the raw samples and rely on the experimental metadata defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze.pix2deg()\n",
    "gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9137f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "gaze_unnested = gaze.clone()\n",
    "gaze_unnested.unnest('position')\n",
    "\n",
    "pm.plotting.tsplot(\n",
    "    gaze_unnested,\n",
    "    channels=['position_x', 'position_y'],\n",
    "    share_y=False,\n",
    "    line_color='darkblue',\n",
    "    n_rows=2,\n",
    "    n_cols=1,\n",
    "    zero_centered_yaxis=False,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze.pos2vel()\n",
    "gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845b738",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "gaze_unnested = gaze.clone()\n",
    "gaze_unnested.unnest('velocity')\n",
    "\n",
    "pm.plotting.tsplot(\n",
    "    gaze_unnested,\n",
    "    channels=['velocity_x', 'velocity_y'],\n",
    "    share_y=False,\n",
    "    line_color='darkblue',\n",
    "    n_rows=2,\n",
    "    n_cols=1,\n",
    "    zero_centered_yaxis=False,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
