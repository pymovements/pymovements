{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81daab99",
   "metadata": {},
   "source": [
    "# Loading Gaze Data\n",
    "\n",
    "Eye trackers export data in a variety of proprietary and semi-standard\n",
    "formats, such as binary `EDF` files, `ASCII`/`ASC` exports, `CSV` or `TSV`\n",
    "tables, or vendor-specific text formats. These files differ in structure,\n",
    "time units, coordinate conventions, and in how samples, events, and metadata\n",
    "are represented. Converting them into a consistent internal representation is\n",
    "therefore a necessary first step before analysis. \n",
    "\n",
    "Loading data into `pymovements` performs this conversion. The loading\n",
    "functions transform heterogeneous eye-tracker exports into a unified data\n",
    "structure by creating a {py:class}`~pymovements.Gaze` object. This object\n",
    "stores time-ordered gaze samples together with the experimental metadata\n",
    "required for meaningful interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ef8b8",
   "metadata": {},
   "source": [
    "## The `Gaze` Object\n",
    "\n",
    "All loading functions in `pymovements` return a {py:class}`~pymovements.Gaze`\n",
    "object. This is the central data structure used throughout the library and\n",
    "serves as a self-contained container for eye-tracking data and its metadata.\n",
    "\n",
    "A `Gaze` object bundles together multiple components of a recording:\n",
    "\n",
    "- **Samples** — the time-ordered gaze signal  \n",
    "- **Events** — optional event annotations (e.g., fixations or saccades)  \n",
    "- **Experiment** — the recording setup definition (screen geometry and eye tracker)  \n",
    "- **Metadata** — additional information provided during import  \n",
    "- **Messages** — optional time-stamped messages from the experiment software  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ed067",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from pymovements import Experiment\n",
    "from pymovements.gaze.io import from_csv\n",
    "\n",
    "experiment = Experiment(\n",
    "    screen_width_px=1280,\n",
    "    screen_height_px=1024,\n",
    "    screen_width_cm=38.0,\n",
    "    screen_height_cm=30.0,\n",
    "    distance_cm=68.0,\n",
    "    origin=\"upper left\",\n",
    "    sampling_rate=1000.0,\n",
    ")\n",
    "\n",
    "gaze = from_csv(\n",
    "    \"../examples/gaze-toy-example.csv\",\n",
    "    experiment=experiment,\n",
    "    time_column=\"timestamp\",\n",
    "    pixel_columns=[\"x\", \"y\"],\n",
    ")\n",
    "\n",
    "gaze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ca8ae",
   "metadata": {},
   "source": [
    "\n",
    "### Samples: The Core Time Series\n",
    "\n",
    "The most important part of the `Gaze` object is the `samples` table. Each row\n",
    "corresponds to one recorded time point, and each column represents a signal\n",
    "channel, such as gaze position, pupil size, velocity, or other measurements.\n",
    "\n",
    "Internally, gaze signals can be stored in **nested component columns**. For\n",
    "example:\n",
    "\n",
    "- Pixel coordinates → column `pixel` with `[x, y]`\n",
    "- Position in degrees of visual angle → column `position`\n",
    "- Velocity → column `velocity`\n",
    "\n",
    "This structure keeps related components together while preserving the full\n",
    "time series.\n",
    "\n",
    "### Events\n",
    "\n",
    "If available, detected or imported eye-movement events are stored separately\n",
    "in `gaze.events`. These are not raw samples but fixations, saccades, or blinks pre-calcualted by the eye-tracker.\n",
    "\n",
    "### Experiment Link\n",
    "\n",
    "Each `Gaze` object can contain an associated\n",
    "{py:class}`~pymovements.Experiment`, which defines screen geometry and\n",
    "sampling rate. This link is essential for interpreting the samples in physical\n",
    "or visual-angle units and for computing time-based measures like velocity. Read more about this in the previous chapter {doc}`The Experiment <experiment>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fe8a4",
   "metadata": {},
   "source": [
    "## Loading CSV Files\n",
    "\n",
    "CSV files are flexible but require explicit column definitions so that\n",
    "`pymovements` knows how to interpret the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5b08f",
   "metadata": {},
   "source": [
    "### Time Information\n",
    "\n",
    "In `pymovements`, timestamps are standardized during loading so that all gaze data share a consistent temporal representation. If `time_unit=None`, milliseconds are assumed. If the `time_unit` is `step,` the {py:class}`~pymovements.Experiment` definition must be specified. \n",
    "\n",
    "If the column containing timestamps is named anything other than `time`, it needs to be specified with the `time_column` paramter to have it renamed internally to `time`.\n",
    "\n",
    "If no `time_column` is provided and no time information can be inferred, a time axis can still be generated if an experiment definition with a sampling rate is available (see Sampling Steps below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c908d",
   "metadata": {},
   "source": [
    "### Defining Gaze Components\n",
    "\n",
    "These parameters define how raw table columns are grouped into structured gaze signals inside the {py:class}`~pymovements.Gaze` object:\n",
    "\n",
    "| Parameter | Description |\n",
    "|----------|-------------|\n",
    "| `pixel_columns` | Gaze positions in **screen pixel coordinates** |\n",
    "| `position_columns` | Gaze positions already in **degrees of visual angle (dva)** |\n",
    "| `velocity_columns` | Gaze velocity components |\n",
    "| `acceleration_columns` | Gaze acceleration components |\n",
    "\n",
    "When provided, these columns are **combined into nested columns** inside the `samples` table:\n",
    "\n",
    "- `pixel` → pixel coordinates  \n",
    "- `position` → dva coordinates  \n",
    "- `velocity` → velocity signal  \n",
    "- `acceleration` → acceleration signal  \n",
    "\n",
    "#### Supported Component Layouts\n",
    "\n",
    "The number of columns determines whether the data are monocular or binocular:\n",
    "\n",
    "| Number of columns | Interpretation | Expected order |\n",
    "|-------------------|---------------|----------------|\n",
    "| **2 columns** | Monocular | x, y |\n",
    "| **4 columns** | Binocular | left x, left y, right x, right y |\n",
    "| **6 columns** | Binocular + cyclopean | left x, left y, right x, right y, cyclopean x, cyclopean y |\n",
    "\n",
    "If the column order differs from this convention, values may be assigned to the wrong eye or component, so it is important to provide columns in the correct sequence.\n",
    "\n",
    "#### Pixel vs. Position Coordinates\n",
    "\n",
    "You typically provide **either**:\n",
    "\n",
    "- `pixel_columns` → if your data are in screen pixels (most common for raw exports)  \n",
    "- `position_columns` → if your data are already converted to degrees of visual angle  \n",
    "\n",
    "If both are provided, `pymovements` keeps both representations, allowing you to switch between coordinate systems without recomputing.\n",
    "\n",
    "Conversions between the two require a valid experiment definition (screen size + viewing distance).\n",
    "\n",
    "#### Using an Experiment\n",
    "\n",
    "Providing an {py:class}`~pymovements.Experiment` connects gaze samples to\n",
    "screen geometry and sampling rate.\n",
    "\n",
    "#### Automatic Column Detection\n",
    "\n",
    "If your column names follow common conventions (e.g., `x`, `y`, `left_x`, `right_y`), you can enable ``auto_column_detect=True``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50676b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze = from_csv(\n",
    "    \"../examples/gaze-toy-example.csv\",\n",
    "    time_column=\"timestamp\",\n",
    "    pixel_columns=[\"x\", \"y\"],\n",
    "    experiment=experiment,\n",
    "    time_unit=\"ms\",\n",
    ")\n",
    "\n",
    "gaze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a128816",
   "metadata": {},
   "source": [
    "## Loading EyeLink ASC Files\n",
    "\n",
    "EyeLink EDF files can be converted to ASC format using `edf2asc`. These files\n",
    "contain gaze samples, events, and metadata. When possible, screen resolution, sampling rate, and tracker metadata are extracted automatically. If an {py:class}`~pymovements.Experiment` is provided, missing values are filled in. Conflicting values raise an error to prevent silent inconsistencies. \n",
    "\n",
    "ASC files can be loaded directly using the {py:func}`~pymovements.gaze.from_asc`. See the {doc}`Parsing SR Research EyeLink Data tutorial <../tutorials/parsing-dataset>` to walk through loading ``*.asc`` files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6621c",
   "metadata": {},
   "source": [
    "## Loading BeGaze Exports\n",
    "\n",
    "BeGaze text exports can be imported directly:\n",
    "\n",
    "```python\n",
    "from pymovements.gaze.io import from_begaze\n",
    "\n",
    "gaze = from_begaze(\"begaze_export.txt\")\n",
    "```\n",
    "\n",
    "Metadata from the file is used to fill missing `Experiment` values when\n",
    "possible. If user-provided values conflict, a warning is issued and the\n",
    "explicit values are kept.\n",
    "\n",
    "## Loading IPC (Feather) Files\n",
    "\n",
    "Previously saved gaze data in IPC (Feather) format can be reloaded quickly:\n",
    "\n",
    "```python\n",
    "from pymovements.gaze.io import from_ipc\n",
    "\n",
    "gaze = from_ipc(\"gaze_data.feather\")\n",
    "```\n",
    "\n",
    "This is the fastest way to restore already processed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1e81d",
   "metadata": {},
   "source": [
    "Now that gaze data has been loaded into a {py:class}`~pymovements.Gaze` object, we will examine its core component, the raw time-series samples, in the next section."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
